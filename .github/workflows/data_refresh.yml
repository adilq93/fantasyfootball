name: Data Pipeline

on: 
  push:
    branches:
     - main

jobs: 
  data-pipeline: 
    name: Data Pipeline 
    runs-on: ubuntu-latest 
    env:
      SEASON_YEAR: 2016
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY}}
      S3_BUCKET_NAME: fantasy-football-pipeline
      AWS_DEFAULT_REGION: us-west-2   
    steps:
    # check-out the repository so job can access all your code
    - uses: actions/checkout@v2
    - name: Set up Python 3.9
      uses: actions/setup-python@v2
      with: 
        python-version: 3.9.7
        token: ${{ secrets.TOKEN_GITHUB }}
    # install poetry 
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        virtualenvs-create: true
        virtualenvs-in-project: true
        installer-parallel: true
    # if an environment already exists, load it; otherwise create a new one 
    - name: Load Cached Virtual Environment
      id: cached-poetry-dependencies
      uses: actions/cache@v2
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
    - name: Install dependencies
      if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
    # if no cache exists, install packages 
      run: poetry install --no-interaction --no-root
    - name: Install AWS CLI
      uses: unfor19/install-aws-cli-action@v1
      with:
        version: 2 
        verbose: false      
    - name: NFL data refresh
      run: |
        source .venv/bin/activate

        python pipeline/collect/collect_calendar.py --season_year $SEASON_YEAR
        python pipeline/process/process_calendar.py --season_year $SEASON_YEAR

        python pipeline/collect/collect_players.py --season_year $SEASON_YEAR
        python pipeline/process/process_players.py --season_year $SEASON_YEAR

        python pipeline/collect/collect_stats.py --season_year $SEASON_YEAR
        python pipeline/process/process_stats.py --season_year $SEASON_YEAR

        python pipeline/collect/collect_weather.py --season_year $SEASON_YEAR
        python pipeline/process/process_weather.py --season_year $SEASON_YEAR     

        python pipeline/collect/collect_injury.py --season_year $SEASON_YEAR
        python pipeline/process/process_injury.py --season_year $SEASON_YEAR

        python pipeline/collect/collect_draft.py --season_year $SEASON_YEAR
        python pipeline/process/process_draft.py --season_year $SEASON_YEAR

        python pipeline/process/process_defense.py --season_year $SEASON_YEAR

        python pipeline/validate/validate_data.py --season_year $SEASON_YEAR --s3_bucket $S3_BUCKET_NAME

        python pipeline/load/load_data.py --season_year $SEASON_YEAR

    - name: Save logs to S3
      run: | 
        aws s3 cp "pipeline/logs/pipeline-run-$(date +'%Y-%m-%d').log" "s3://$S3_BUCKET_NAME/logs/"

    - name: Commit Changes
      run: |
        git config --global user.name 'MarkLeBoeuf'
        git config --global user.email 'markleboeuf2015@u.northwestern.edu'
        git add src/fantasyfootball/datasets/*.gz        
        git commit -m "data: auto update historical NFL data"

    - name: Push Changes
      uses: ad-m/github-push-action@master


